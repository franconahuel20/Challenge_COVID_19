{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge TÃ©cnico â€“ Sr Data Engineer\n",
        "\n",
        "Â¡Felicitaciones!\n",
        "\n",
        "Si estÃ¡s leyendo esto, es porque llegaste a una etapa muy importante de nuestro proceso de selecciÃ³n.  \n",
        "Te invitamos a desarrollar nuestro **Challenge TÃ©cnico** para la posiciÃ³n de **Sr Data Engineer**.\n",
        "\n",
        "---\n",
        "\n",
        "## Â¿Por quÃ© esta etapa es importante?\n",
        "\n",
        "Porque nos ayuda a realizar la prÃ³xima etapa (entrevista tÃ©cnica) con mayor objetividad, pero principalmente nos aporta informaciÃ³n muy valiosa sobre tus **hard skills**.\n",
        "\n",
        "---\n",
        "\n",
        "## Â¿CuÃ¡nto tiempo tengo para realizar el Challenge?\n",
        "\n",
        "Tienes **5 dÃ­as corridos** para realizarlo.  \n",
        "EstÃ¡ pensado para invertir aproximadamente **1 hora por dÃ­a**, considerando que tambiÃ©n tienes otras responsabilidades laborales y personales.\n",
        "\n",
        "---\n",
        "\n",
        "## Â¿QuÃ© sucede si no realizo el Challenge?\n",
        "\n",
        "Lamentablemente, no podremos continuar con el proceso, ya que se trata de una de las instancias **mÃ¡s importantes y definitorias**.\n",
        "\n",
        "---\n",
        "\n",
        "Â¡Mucho Ã©xito!\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“Œ Challenge\n",
        "\n",
        "### 1. Dataset\n",
        "- Descargar el dataset **Covid-19** desde Kaggle:  \n",
        "  https://www.kaggle.com/datasets/imdevskp/corona-virus-report\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Procesamiento con Spark\n",
        "- Cargar los datasets utilizando **Apache Spark**.\n",
        "- Mantener los datos en formato **Parquet**.\n",
        "- Utilizar **paralelismo y RDD**.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Procesamiento con Pandas\n",
        "- Cargar los datasets utilizando **Pandas**.\n",
        "- Mantener los datos en formato **Parquet**.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Modelado\n",
        "- Desarrollar un **diagrama DER (Diagrama Entidad-RelaciÃ³n)** del modelado de datos.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¦ Entregables\n",
        "\n",
        "- Entregar el **proceso y los datos en un archivo ZIP**.\n",
        "  - No es necesario entregar el ambiente armado.\n",
        "- El desafÃ­o **puede publicarse en GitHub** (opcional).\n",
        "\n",
        "---\n",
        "\n",
        "## âš™ï¸ Consideraciones TÃ©cnicas\n",
        "\n",
        "### Spark\n",
        "- Para realizar el desafÃ­o con Spark es necesario instalar un **Spark Cluster**.\n",
        "- Recursos Ãºtiles:\n",
        "  - https://towardsdatascience.com/spark-on-windows-a-getting-started-guide-11dc44412164\n",
        "  - https://www.geeksforgeeks.org/install-apache-spark-in-a-standalone-mode-on-windows/\n",
        "  - https://medium.com/codex/setup-a-spark-cluster-step-by-step-in-10-minutes-922c06f8e2b1\n",
        "- Para Spark, **se debe utilizar RDD**.\n",
        "\n",
        "---\n",
        "\n",
        "### CÃ³digo y DiseÃ±o\n",
        "- El cÃ³digo debe:\n",
        "  - Tener **comentarios en las partes importantes**.\n",
        "  - Estar desarrollado de forma **orientada a objetos** (uso de **clases**, incluso dentro de notebooks).\n",
        "\n",
        "---\n",
        "\n",
        "### Calidad de Datos\n",
        "- Tratar correctamente los **tipos de datos en la Ãºltima capa**.\n",
        "  - Ejemplo: todo lo que sea numÃ©rico debe ser **numÃ©rico**, no string.\n",
        "\n",
        "---\n",
        "\n",
        "### Control de Datos\n",
        "- Implementar un **control de datos**:\n",
        "  - Para cada procesamiento, obtener **solo las novedades**.\n",
        "  - Utilizar un esquema de **offset / delta de datos** (no reprocesar todo).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§‘â€ðŸ’» Lenguaje\n",
        "- El desafÃ­o puede realizarse en **Python o Scala**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Nw4awPH-D6Jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "base = \"/content/covid_challenge\"\n",
        "\n",
        "for folder in [\"parquet_spark\", \"parquet_pandas\", \"state\"]:\n",
        "    path = os.path.join(base, folder)\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)\n"
      ],
      "metadata": {
        "id": "jbACQH8kRZN0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpxPyUNmDBNC",
        "outputId": "8052b571-1c2c-4731-9196-e7c0bee11a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Kaggle credentials already present.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# Colab: Sr Data Engineer Challenge - End-to-end Implementation\n",
        "# Dataset: https://www.kaggle.com/datasets/imdevskp/corona-virus-report\n",
        "#\n",
        "# Key goals (per PDF):\n",
        "# - Download dataset from Kaggle\n",
        "# - Load datasets with Spark (use parallelism + RDD) and store as Parquet\n",
        "# - Load datasets with Pandas and store as Parquet\n",
        "# - OOP approach (classes in notebook)\n",
        "# - Type handling in the last layer\n",
        "# - Delta/offset control: process only \"novelties\" (new rows per run)\n",
        "# - Comment important parts in English and keep code clean / SOLID\n",
        "# ============================================================\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import shutil\n",
        "import hashlib\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Tuple, Iterable\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# ----------------------------\n",
        "# 1) Install dependencies\n",
        "# ----------------------------\n",
        "!pip -q install kaggle pyarrow fastparquet\n",
        "\n",
        "# Spark in Colab\n",
        "!apt-get -qq update\n",
        "!apt-get -qq install -y openjdk-11-jdk-headless > /dev/null\n",
        "!pip -q install pyspark\n",
        "\n",
        "# ----------------------------\n",
        "# 2) Kaggle auth setup\n",
        "# ----------------------------\n",
        "from google.colab import files\n",
        "\n",
        "def _ensure_kaggle_credentials(kaggle_dir: str = \"/root/.kaggle\") -> None:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - kaggle_dir: folder where kaggle.json must exist\n",
        "    Output:\n",
        "      - None. Ensures Kaggle credentials are present with correct permissions.\n",
        "    \"\"\"\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "    kaggle_json_path = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "\n",
        "    if os.path.exists(kaggle_json_path):\n",
        "        os.chmod(kaggle_json_path, 0o600)\n",
        "        print(\"Kaggle credentials already present.\")\n",
        "        return\n",
        "\n",
        "    print(\"Upload your Kaggle API token file (kaggle.json) to continue.\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        raise RuntimeError(\"No file uploaded. Please click 'Choose files' and select kaggle.json.\")\n",
        "\n",
        "    # If the user uploads kaggle.json with a different name (common in Colab),\n",
        "    # we pick the first .json file and rename it to kaggle.json.\n",
        "    json_files = [name for name in uploaded.keys() if name.lower().endswith(\".json\")]\n",
        "    if not json_files:\n",
        "        raise RuntimeError(f\"Uploaded file(s) are not JSON: {list(uploaded.keys())}. Please upload kaggle.json.\")\n",
        "\n",
        "    uploaded_name = json_files[0]\n",
        "\n",
        "    # Move/rename to the expected location\n",
        "    shutil.move(uploaded_name, kaggle_json_path)\n",
        "    os.chmod(kaggle_json_path, 0o600)\n",
        "\n",
        "    print(f\"Kaggle credentials ready: {kaggle_json_path}\")\n",
        "\n",
        "_ensure_kaggle_credentials()\n",
        "\n",
        "# ----------------------------\n",
        "# 3) Spark imports\n",
        "# ----------------------------\n",
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Clean Code / SOLID: Core domain model\n",
        "# ============================================================\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class AppConfig:\n",
        "    \"\"\"\n",
        "    Input:\n",
        "      - base_dir: root folder for data outputs\n",
        "    Output:\n",
        "      - Provides normalized paths for raw/processed/state layers.\n",
        "    \"\"\"\n",
        "    base_dir: str = \"/content/covid_challenge\"\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self) -> str:\n",
        "        return os.path.join(self.base_dir, \"raw\")\n",
        "\n",
        "    @property\n",
        "    def spark_parquet_dir(self) -> str:\n",
        "        return os.path.join(self.base_dir, \"parquet_spark\")\n",
        "\n",
        "    @property\n",
        "    def pandas_parquet_dir(self) -> str:\n",
        "        return os.path.join(self.base_dir, \"parquet_pandas\")\n",
        "\n",
        "    @property\n",
        "    def state_dir(self) -> str:\n",
        "        return os.path.join(self.base_dir, \"state\")\n",
        "\n",
        "    def ensure_dirs(self) -> None:\n",
        "        os.makedirs(self.raw_dir, exist_ok=True)\n",
        "        os.makedirs(self.spark_parquet_dir, exist_ok=True)\n",
        "        os.makedirs(self.pandas_parquet_dir, exist_ok=True)\n",
        "        os.makedirs(self.state_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "class SparkSessionFactory:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Create and configure SparkSession.\n",
        "    \"\"\"\n",
        "\n",
        "    def create(self, app_name: str = \"SrDataEngineerCovidChallenge\") -> SparkSession:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - app_name: Spark application name\n",
        "        Output:\n",
        "          - SparkSession ready to use\n",
        "        \"\"\"\n",
        "        spark = (\n",
        "            SparkSession.builder\n",
        "            .appName(app_name)\n",
        "            .master(\"local[*]\")\n",
        "            # Practical settings for local/colab execution\n",
        "            .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "            .config(\"spark.default.parallelism\", \"8\")\n",
        "            .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
        "            .getOrCreate()\n",
        "        )\n",
        "        spark.sparkContext.setLogLevel(\"WARN\")\n",
        "        return spark\n",
        "\n",
        "\n",
        "class KaggleDownloader:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Download and unzip Kaggle dataset into raw_dir.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset: str) -> None:\n",
        "        self._dataset = dataset\n",
        "\n",
        "    def download_and_extract(self, raw_dir: str) -> str:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - raw_dir: destination folder for raw files\n",
        "        Output:\n",
        "          - path to the folder containing extracted CSVs\n",
        "        \"\"\"\n",
        "        # Optional: keep raw_dir clean to avoid picking an old zip\n",
        "        for f in os.listdir(raw_dir):\n",
        "          if f.endswith(\".zip\"):\n",
        "            os.remove(os.path.join(raw_dir, f))\n",
        "\n",
        "        os.makedirs(raw_dir, exist_ok=True)\n",
        "\n",
        "        # Download full dataset zip (no -f, because -f is for a specific file inside the dataset)\n",
        "        !kaggle datasets download -d {self._dataset} -p {raw_dir} -q\n",
        "\n",
        "        # Kaggle names the zip like \"<dataset-slug>.zip\"\n",
        "        zips = [os.path.join(raw_dir, f) for f in os.listdir(raw_dir) if f.endswith(\".zip\")]\n",
        "        if not zips:\n",
        "            raise RuntimeError(\"Dataset zip not found after Kaggle download.\")\n",
        "\n",
        "        zip_path = zips[0]\n",
        "\n",
        "        extract_dir = os.path.join(raw_dir, \"extracted\")\n",
        "        os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "        import zipfile\n",
        "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "            zf.extractall(extract_dir)\n",
        "\n",
        "        print(f\"Extracted dataset to: {extract_dir}\")\n",
        "        return extract_dir\n",
        "\n",
        "\n",
        "@dataclass(frozen=True)\n",
        "class DatasetSpec:\n",
        "    \"\"\"\n",
        "    Defines how to process a dataset.\n",
        "    \"\"\"\n",
        "    name: str                 # logical name (used for output folders)\n",
        "    csv_path: str             # absolute path to the CSV\n",
        "    key_columns: List[str]    # columns used to define uniqueness for delta control\n",
        "    schema: Dict[str, str]    # last-layer types: spark SQL types as strings (e.g., 'int', 'double', 'date')\n",
        "\n",
        "\n",
        "class DatasetCatalog:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Discover CSVs in extracted folder and produce DatasetSpec objects.\n",
        "    Notes:\n",
        "      - Key columns are chosen to be stable and practical for delta tracking.\n",
        "      - Type casting is applied at the 'last layer' before writing Parquet.\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, extracted_dir: str) -> List[DatasetSpec]:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - extracted_dir: folder with CSV files\n",
        "        Output:\n",
        "          - list of DatasetSpec for each known dataset\n",
        "        \"\"\"\n",
        "        files = {f.lower(): os.path.join(extracted_dir, f) for f in os.listdir(extracted_dir) if f.lower().endswith(\".csv\")}\n",
        "\n",
        "        def pick(*candidates: str) -> Optional[str]:\n",
        "            for c in candidates:\n",
        "                if c.lower() in files:\n",
        "                    return files[c.lower()]\n",
        "            return None\n",
        "\n",
        "        # Known files in this Kaggle dataset (can change slightly; handle gracefully)\n",
        "        specs: List[DatasetSpec] = []\n",
        "\n",
        "        # Helper schemas (keep explicit to show type handling)\n",
        "        date_schema = {\n",
        "            \"Date\": \"date\",\n",
        "        }\n",
        "\n",
        "        # covid_19_clean_complete.csv\n",
        "        p = pick(\"covid_19_clean_complete.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"covid_19_clean_complete\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"Date\", \"Country/Region\", \"Province/State\"],\n",
        "                schema={\n",
        "                    \"Date\": \"date\",\n",
        "                    \"Province/State\": \"string\",\n",
        "                    \"Country/Region\": \"string\",\n",
        "                    \"Lat\": \"double\",\n",
        "                    \"Long\": \"double\",\n",
        "                    \"Confirmed\": \"long\",\n",
        "                    \"Deaths\": \"long\",\n",
        "                    \"Recovered\": \"long\",\n",
        "                    \"Active\": \"long\",\n",
        "                    \"WHO Region\": \"string\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # country_wise_latest.csv\n",
        "        p = pick(\"country_wise_latest.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"country_wise_latest\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"Country/Region\"],\n",
        "                schema={\n",
        "                    \"Country/Region\": \"string\",\n",
        "                    \"Confirmed\": \"long\",\n",
        "                    \"Deaths\": \"long\",\n",
        "                    \"Recovered\": \"long\",\n",
        "                    \"Active\": \"long\",\n",
        "                    \"New cases\": \"long\",\n",
        "                    \"New deaths\": \"long\",\n",
        "                    \"New recovered\": \"long\",\n",
        "                    \"Deaths / 100 Cases\": \"double\",\n",
        "                    \"Recovered / 100 Cases\": \"double\",\n",
        "                    \"Deaths / 100 Recovered\": \"double\",\n",
        "                    \"Confirmed last week\": \"long\",\n",
        "                    \"1 week change\": \"long\",\n",
        "                    \"1 week % increase\": \"double\",\n",
        "                    \"WHO Region\": \"string\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # day_wise.csv\n",
        "        p = pick(\"day_wise.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"day_wise\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"Date\"],\n",
        "                schema={\n",
        "                    \"Date\": \"date\",\n",
        "                    \"Confirmed\": \"long\",\n",
        "                    \"Deaths\": \"long\",\n",
        "                    \"Recovered\": \"long\",\n",
        "                    \"Active\": \"long\",\n",
        "                    \"New cases\": \"long\",\n",
        "                    \"New deaths\": \"long\",\n",
        "                    \"New recovered\": \"long\",\n",
        "                    \"Deaths / 100 Cases\": \"double\",\n",
        "                    \"Recovered / 100 Cases\": \"double\",\n",
        "                    \"Deaths / 100 Recovered\": \"double\",\n",
        "                    \"No. of countries\": \"long\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # full_grouped.csv\n",
        "        p = pick(\"full_grouped.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"full_grouped\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"Date\", \"Country/Region\"],\n",
        "                schema={\n",
        "                    \"Date\": \"date\",\n",
        "                    \"Country/Region\": \"string\",\n",
        "                    \"Confirmed\": \"long\",\n",
        "                    \"Deaths\": \"long\",\n",
        "                    \"Recovered\": \"long\",\n",
        "                    \"Active\": \"long\",\n",
        "                    \"New cases\": \"long\",\n",
        "                    \"New deaths\": \"long\",\n",
        "                    \"New recovered\": \"long\",\n",
        "                    \"WHO Region\": \"string\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # usa_county_wise.csv\n",
        "        p = pick(\"usa_county_wise.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"usa_county_wise\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"UID\"],  # UID is the most stable unique identifier in this file\n",
        "                schema={\n",
        "                    \"UID\": \"long\",\n",
        "                    \"iso2\": \"string\",\n",
        "                    \"iso3\": \"string\",\n",
        "                    \"code3\": \"string\",\n",
        "                    \"FIPS\": \"string\",\n",
        "                    \"Admin2\": \"string\",\n",
        "                    \"Province_State\": \"string\",\n",
        "                    \"Country_Region\": \"string\",\n",
        "                    \"Lat\": \"double\",\n",
        "                    \"Long_\": \"double\",\n",
        "                    \"Combined_Key\": \"string\",\n",
        "                    \"Confirmed\": \"long\",\n",
        "                    \"Deaths\": \"long\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        # worldometer_data.csv\n",
        "        p = pick(\"worldometer_data.csv\")\n",
        "        if p:\n",
        "            specs.append(DatasetSpec(\n",
        "                name=\"worldometer_data\",\n",
        "                csv_path=p,\n",
        "                key_columns=[\"Country/Region\"],\n",
        "                schema={\n",
        "                    \"Country/Region\": \"string\",\n",
        "                    \"Continent\": \"string\",\n",
        "                    \"Population\": \"long\",\n",
        "                    \"TotalCases\": \"long\",\n",
        "                    \"NewCases\": \"long\",\n",
        "                    \"TotalDeaths\": \"long\",\n",
        "                    \"NewDeaths\": \"long\",\n",
        "                    \"TotalRecovered\": \"long\",\n",
        "                    \"NewRecovered\": \"long\",\n",
        "                    \"ActiveCases\": \"long\",\n",
        "                    \"Serious,Critical\": \"long\",\n",
        "                    \"Tot Cases/1M pop\": \"double\",\n",
        "                    \"Deaths/1M pop\": \"double\",\n",
        "                    \"TotalTests\": \"long\",\n",
        "                    \"Tests/1M pop\": \"double\",\n",
        "                    \"WHO Region\": \"string\",\n",
        "                }\n",
        "            ))\n",
        "\n",
        "        if not specs:\n",
        "            raise RuntimeError(\"No known CSVs found in extracted dataset folder.\")\n",
        "\n",
        "        return specs\n",
        "\n",
        "\n",
        "class SparkSchemaCaster:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Apply strict typing at the last layer (before writing Parquet).\n",
        "    \"\"\"\n",
        "\n",
        "    _TYPE_MAP: Dict[str, T.DataType] = {\n",
        "        \"string\": T.StringType(),\n",
        "        \"int\": T.IntegerType(),\n",
        "        \"long\": T.LongType(),\n",
        "        \"double\": T.DoubleType(),\n",
        "        \"date\": T.DateType(),\n",
        "        \"timestamp\": T.TimestampType(),\n",
        "        \"boolean\": T.BooleanType(),\n",
        "    }\n",
        "\n",
        "    def cast(self, df: DataFrame, schema: Dict[str, str]) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - df: Spark DataFrame with raw columns\n",
        "          - schema: dict of column -> type (string names)\n",
        "        Output:\n",
        "          - DataFrame with casted columns (when present)\n",
        "        \"\"\"\n",
        "        out = df\n",
        "\n",
        "        # Important: CSV inputs often contain columns with spaces, slashes and dots.\n",
        "        # We must always escape column names using backticks to avoid Spark parsing issues.\n",
        "        for col_name, type_name in schema.items():\n",
        "            if col_name not in out.columns:\n",
        "                continue\n",
        "\n",
        "            if type_name not in self._TYPE_MAP:\n",
        "                raise ValueError(f\"Unsupported type '{type_name}' for column '{col_name}'\")\n",
        "\n",
        "            spark_type = self._TYPE_MAP[type_name]\n",
        "\n",
        "            # Always use escaped column reference\n",
        "            c = F.col(f\"`{col_name}`\")\n",
        "\n",
        "            if type_name == \"date\":\n",
        "                out = out.withColumn(col_name, F.to_date(c))\n",
        "            else:\n",
        "                out = out.withColumn(\n",
        "                    col_name,\n",
        "                    F.regexp_replace(c.cast(\"string\"), \",\", \"\").cast(spark_type)\n",
        "                )\n",
        "\n",
        "        return out\n",
        "\n",
        "class DeltaStateStore:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Persist and load the \"already processed keys\" for each dataset.\n",
        "    Design choice:\n",
        "      - We store a compact SHA-256 hash of the composite key to minimize storage.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, spark: SparkSession, state_dir: str) -> None:\n",
        "        self._spark = spark\n",
        "        self._state_dir = state_dir\n",
        "        os.makedirs(self._state_dir, exist_ok=True)\n",
        "\n",
        "    def _state_path(self, dataset_name: str) -> str:\n",
        "        return os.path.join(self._state_dir, f\"{dataset_name}_seen_keys.parquet\")\n",
        "\n",
        "    def load_seen_keys(self, dataset_name: str) -> Optional[DataFrame]:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - dataset_name\n",
        "        Output:\n",
        "          - DataFrame with column 'row_key_hash' if exists; otherwise None\n",
        "        \"\"\"\n",
        "        path = self._state_path(dataset_name)\n",
        "        if not os.path.exists(path):\n",
        "            return None\n",
        "        return self._spark.read.parquet(path)\n",
        "\n",
        "    def save_seen_keys(self, dataset_name: str, keys_df: DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - dataset_name\n",
        "          - keys_df: DataFrame with column 'row_key_hash'\n",
        "        Output:\n",
        "          - None. Writes Parquet overwriting previous state.\n",
        "        \"\"\"\n",
        "        path = self._state_path(dataset_name)\n",
        "        keys_df.select(\"row_key_hash\").dropDuplicates().write.mode(\"overwrite\").parquet(path)\n",
        "\n",
        "\n",
        "class SparkCovidIngestor:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Read CSV with Spark, use RDD for parallelism, apply delta logic, cast types, write Parquet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        spark: SparkSession,\n",
        "        caster: SparkSchemaCaster,\n",
        "        state_store: DeltaStateStore,\n",
        "        output_dir: str,\n",
        "    ) -> None:\n",
        "        self._spark = spark\n",
        "        self._caster = caster\n",
        "        self._state_store = state_store\n",
        "        self._output_dir = output_dir\n",
        "        os.makedirs(self._output_dir, exist_ok=True)\n",
        "\n",
        "    def _compute_row_key_hash(self, df: DataFrame, key_columns: List[str]) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - df: DataFrame with raw columns\n",
        "          - key_columns: list of columns that define uniqueness\n",
        "        Output:\n",
        "          - DataFrame with an extra 'row_key_hash' column\n",
        "        \"\"\"\n",
        "        # Concatenate keys safely (null-safe) and hash.\n",
        "        # Using SHA-256 to reduce collision risk.\n",
        "        concat_expr = F.concat_ws(\n",
        "            \"||\",\n",
        "            *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in key_columns]\n",
        "        )\n",
        "        return df.withColumn(\"row_key_hash\", F.sha2(concat_expr, 256))\n",
        "\n",
        "    def _read_csv_with_rdd(self, csv_path: str) -> DataFrame:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - csv_path: path to CSV file\n",
        "        Output:\n",
        "          - DataFrame loaded from CSV using an RDD-based approach for the file content\n",
        "        Why RDD here:\n",
        "          - The challenge explicitly requests using RDD.\n",
        "          - We read the file as text via SparkContext.textFile (RDD), then parse with Spark CSV reader.\n",
        "        \"\"\"\n",
        "        sc = self._spark.sparkContext\n",
        "\n",
        "        # RDD parallel read of file content\n",
        "        rdd = sc.textFile(csv_path)\n",
        "\n",
        "        # Create a DataFrame from the RDD lines via spark.read.csv on an RDD\n",
        "        # Note: Spark supports reading from an RDD[String] with header inference.\n",
        "        df = (\n",
        "            self._spark.read\n",
        "            .option(\"header\", True)\n",
        "            .option(\"inferSchema\", False)  # we enforce types later\n",
        "            .option(\"multiLine\", False)\n",
        "            .csv(rdd)\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def ingest(self, spec: DatasetSpec) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - spec: DatasetSpec defining file, keys, schema\n",
        "        Output:\n",
        "          - (total_rows_read, delta_rows_written)\n",
        "        \"\"\"\n",
        "        df_raw = self._read_csv_with_rdd(spec.csv_path)\n",
        "        total_rows = df_raw.count()\n",
        "\n",
        "        # Delta logic: keep only rows whose composite key hash was never seen before\n",
        "        df_with_keys = self._compute_row_key_hash(df_raw, spec.key_columns)\n",
        "        seen = self._state_store.load_seen_keys(spec.name)\n",
        "\n",
        "        if seen is not None:\n",
        "            df_delta = df_with_keys.join(seen, on=\"row_key_hash\", how=\"left_anti\")\n",
        "        else:\n",
        "            df_delta = df_with_keys\n",
        "\n",
        "        # Cast at last layer (before writing)\n",
        "        df_typed = self._caster.cast(df_delta, spec.schema)\n",
        "\n",
        "        # Write delta parquet (append)\n",
        "        out_path = os.path.join(self._output_dir, spec.name)\n",
        "        df_typed.write.mode(\"append\").parquet(out_path)\n",
        "\n",
        "        delta_rows = df_typed.count()\n",
        "\n",
        "        # Update state with newly processed keys\n",
        "        new_keys = df_with_keys.select(\"row_key_hash\").dropDuplicates()\n",
        "        if seen is not None:\n",
        "            merged = seen.unionByName(new_keys).dropDuplicates([\"row_key_hash\"])\n",
        "        else:\n",
        "            merged = new_keys\n",
        "        self._state_store.save_seen_keys(spec.name, merged)\n",
        "\n",
        "        return total_rows, delta_rows\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Pandas pipeline (separate responsibility, same outputs)\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "class PandasSchemaCaster:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Apply final typing in Pandas pipeline before writing Parquet.\n",
        "    \"\"\"\n",
        "\n",
        "    def cast(self, pdf: pd.DataFrame, schema: Dict[str, str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - pdf: Pandas DataFrame\n",
        "          - schema: dict of column -> type string\n",
        "        Output:\n",
        "          - DataFrame with casted columns (when possible)\n",
        "        \"\"\"\n",
        "        out = pdf.copy()\n",
        "\n",
        "        for col, type_name in schema.items():\n",
        "            if col not in out.columns:\n",
        "                continue\n",
        "\n",
        "            if type_name in (\"int\", \"long\"):\n",
        "                out[col] = pd.to_numeric(out[col].astype(str).str.replace(\",\", \"\"), errors=\"coerce\").astype(\"Int64\")\n",
        "            elif type_name == \"double\":\n",
        "                out[col] = pd.to_numeric(out[col].astype(str).str.replace(\",\", \"\"), errors=\"coerce\")\n",
        "            elif type_name == \"date\":\n",
        "                out[col] = pd.to_datetime(out[col], errors=\"coerce\").dt.date\n",
        "            elif type_name == \"boolean\":\n",
        "                out[col] = out[col].astype(\"boolean\")\n",
        "            else:\n",
        "                out[col] = out[col].astype(\"string\")\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class PandasDeltaStateStore:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Persist seen keys for Pandas pipeline (same idea as Spark state).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, state_dir: str) -> None:\n",
        "        self._state_dir = state_dir\n",
        "        os.makedirs(self._state_dir, exist_ok=True)\n",
        "\n",
        "    def _state_path(self, dataset_name: str) -> str:\n",
        "        return os.path.join(self._state_dir, f\"{dataset_name}_seen_keys_pandas.parquet\")\n",
        "\n",
        "    def load_seen(self, dataset_name: str) -> Optional[pd.DataFrame]:\n",
        "        path = self._state_path(dataset_name)\n",
        "        if not os.path.exists(path):\n",
        "            return None\n",
        "        return pd.read_parquet(path)\n",
        "\n",
        "    def save_seen(self, dataset_name: str, seen_df: pd.DataFrame) -> None:\n",
        "        path = self._state_path(dataset_name)\n",
        "        seen_df[[\"row_key_hash\"]].drop_duplicates().to_parquet(path, index=False)\n",
        "\n",
        "\n",
        "class PandasCovidIngestor:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Read CSV with Pandas, apply delta logic, cast types, write Parquet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        caster: PandasSchemaCaster,\n",
        "        state_store: PandasDeltaStateStore,\n",
        "        output_dir: str,\n",
        "    ) -> None:\n",
        "        self._caster = caster\n",
        "        self._state_store = state_store\n",
        "        self._output_dir = output_dir\n",
        "        os.makedirs(self._output_dir, exist_ok=True)\n",
        "\n",
        "    def _compute_row_key_hash(self, pdf: pd.DataFrame, key_columns: List[str]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - pdf: raw Pandas DataFrame\n",
        "          - key_columns: columns that define uniqueness\n",
        "        Output:\n",
        "          - DataFrame with an extra 'row_key_hash' column\n",
        "        \"\"\"\n",
        "        out = pdf.copy()\n",
        "\n",
        "        # Null-safe string concatenation for composite keys\n",
        "        # We keep it simple and deterministic so it matches run-to-run.\n",
        "        def _row_key(row: pd.Series) -> str:\n",
        "            parts = []\n",
        "            for c in key_columns:\n",
        "                v = row.get(c, \"\")\n",
        "                if pd.isna(v):\n",
        "                    v = \"\"\n",
        "                parts.append(str(v))\n",
        "            return \"||\".join(parts)\n",
        "\n",
        "        keys = out.apply(_row_key, axis=1)\n",
        "        out[\"row_key_hash\"] = keys.apply(lambda s: hashlib.sha256(s.encode(\"utf-8\")).hexdigest())\n",
        "        return out\n",
        "\n",
        "    def ingest(self, spec: DatasetSpec) -> Tuple[int, int]:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - spec: DatasetSpec defining file, keys, schema\n",
        "        Output:\n",
        "          - (total_rows_read, delta_rows_written)\n",
        "        \"\"\"\n",
        "        pdf_raw = pd.read_csv(spec.csv_path)\n",
        "        total_rows = int(len(pdf_raw))\n",
        "\n",
        "        pdf_with_keys = self._compute_row_key_hash(pdf_raw, spec.key_columns)\n",
        "        seen = self._state_store.load_seen(spec.name)\n",
        "\n",
        "        if seen is not None and \"row_key_hash\" in seen.columns:\n",
        "            # Keep only novelties (rows never processed before)\n",
        "            pdf_delta = pdf_with_keys[~pdf_with_keys[\"row_key_hash\"].isin(set(seen[\"row_key_hash\"].tolist()))].copy()\n",
        "        else:\n",
        "            pdf_delta = pdf_with_keys\n",
        "\n",
        "        # Type enforcement at the last layer (before writing parquet)\n",
        "        pdf_typed = self._caster.cast(pdf_delta, spec.schema)\n",
        "\n",
        "        out_path = os.path.join(self._output_dir, spec.name)\n",
        "        os.makedirs(out_path, exist_ok=True)\n",
        "\n",
        "        # Write one parquet file per run to keep append semantics simple in Colab\n",
        "        run_id = hashlib.md5(str(pd.Timestamp.utcnow()).encode(\"utf-8\")).hexdigest()[:10]\n",
        "        parquet_path = os.path.join(out_path, f\"part_{run_id}.parquet\")\n",
        "        pdf_typed.to_parquet(parquet_path, index=False)\n",
        "\n",
        "        delta_rows = int(len(pdf_typed))\n",
        "\n",
        "        # Update state (append new keys into the \"seen\" set)\n",
        "        new_keys = pdf_with_keys[[\"row_key_hash\"]].drop_duplicates()\n",
        "\n",
        "        if seen is not None and \"row_key_hash\" in seen.columns:\n",
        "            merged = pd.concat([seen[[\"row_key_hash\"]], new_keys], ignore_index=True).drop_duplicates()\n",
        "        else:\n",
        "            merged = new_keys\n",
        "\n",
        "        self._state_store.save_seen(spec.name, merged)\n",
        "\n",
        "        return total_rows, delta_rows\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Orchestration layer (keeps notebook clean and testable)\n",
        "# ============================================================\n",
        "\n",
        "class PipelineRunner:\n",
        "    \"\"\"\n",
        "    Single Responsibility:\n",
        "      - Orchestrate the end-to-end pipeline steps using injected dependencies.\n",
        "    Design:\n",
        "      - Keeps the \"main\" flow readable and avoids mixing I/O with business logic.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: AppConfig,\n",
        "        downloader: KaggleDownloader,\n",
        "        catalog: DatasetCatalog,\n",
        "        spark_factory: SparkSessionFactory,\n",
        "    ) -> None:\n",
        "        self._config = config\n",
        "        self._downloader = downloader\n",
        "        self._catalog = catalog\n",
        "        self._spark_factory = spark_factory\n",
        "\n",
        "    def run(self) -> None:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - None (uses config and injected collaborators)\n",
        "        Output:\n",
        "          - None. Writes parquet outputs and state to disk.\n",
        "        \"\"\"\n",
        "        self._config.ensure_dirs()\n",
        "\n",
        "        extracted_dir = self._downloader.download_and_extract(self._config.raw_dir)\n",
        "        specs = self._catalog.build(extracted_dir)\n",
        "\n",
        "        spark = self._spark_factory.create()\n",
        "\n",
        "        try:\n",
        "            # Spark pipeline components\n",
        "            spark_caster = SparkSchemaCaster()\n",
        "            spark_state = DeltaStateStore(spark=spark, state_dir=self._config.state_dir)\n",
        "            spark_ingestor = SparkCovidIngestor(\n",
        "                spark=spark,\n",
        "                caster=spark_caster,\n",
        "                state_store=spark_state,\n",
        "                output_dir=self._config.spark_parquet_dir,\n",
        "            )\n",
        "\n",
        "            # Pandas pipeline components\n",
        "            pandas_caster = PandasSchemaCaster()\n",
        "            pandas_state = PandasDeltaStateStore(state_dir=self._config.state_dir)\n",
        "            pandas_ingestor = PandasCovidIngestor(\n",
        "                caster=pandas_caster,\n",
        "                state_store=pandas_state,\n",
        "                output_dir=self._config.pandas_parquet_dir,\n",
        "            )\n",
        "\n",
        "            print(\"============================================================\")\n",
        "            print(\"Running ingestion (Spark + RDD) and ingestion (Pandas)\")\n",
        "            print(\"Delta logic: only novelties are appended each run\")\n",
        "            print(\"============================================================\")\n",
        "\n",
        "            spark_report = []\n",
        "            pandas_report = []\n",
        "\n",
        "            for spec in specs:\n",
        "                # Spark ingestion\n",
        "                total_s, delta_s = spark_ingestor.ingest(spec)\n",
        "                spark_report.append((spec.name, total_s, delta_s))\n",
        "\n",
        "                # Pandas ingestion\n",
        "                total_p, delta_p = pandas_ingestor.ingest(spec)\n",
        "                pandas_report.append((spec.name, total_p, delta_p))\n",
        "\n",
        "            print(\"\\n--- Spark report (RDD-based CSV read) ---\")\n",
        "            for name, total, delta in spark_report:\n",
        "                print(f\"[Spark] {name}: read={total} | delta_written={delta}\")\n",
        "\n",
        "            print(\"\\n--- Pandas report ---\")\n",
        "            for name, total, delta in pandas_report:\n",
        "                print(f\"[Pandas] {name}: read={total} | delta_written={delta}\")\n",
        "\n",
        "            # Optional: quick sanity check showing one dataset schema after typing\n",
        "            self._print_sample_schema(spark, specs)\n",
        "\n",
        "            # Create deliverable zip\n",
        "            zip_path = self._build_zip_deliverable()\n",
        "            print(f\"\\nDeliverable ZIP created at: {zip_path}\")\n",
        "\n",
        "            # Provide DER diagram (as text artifact)\n",
        "            self._print_der_diagram()\n",
        "\n",
        "        finally:\n",
        "            # Always close Spark cleanly\n",
        "            print(\"\\nStopping Spark session...\")\n",
        "            spark.stop()\n",
        "            print(\"Spark stopped.\")\n",
        "\n",
        "    def _print_sample_schema(self, spark: SparkSession, specs: List[DatasetSpec]) -> None:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - spark: active SparkSession\n",
        "          - specs: dataset specs list\n",
        "        Output:\n",
        "          - None. Prints one schema as a quick verification.\n",
        "        \"\"\"\n",
        "        # Pick the first dataset that exists in Spark output\n",
        "        if not specs:\n",
        "            return\n",
        "\n",
        "        candidate = specs[0].name\n",
        "        path = os.path.join(self._config.spark_parquet_dir, candidate)\n",
        "\n",
        "        if os.path.exists(path):\n",
        "            print(f\"\\nSchema check (Spark parquet): {candidate}\")\n",
        "            df = spark.read.parquet(path)\n",
        "            df.printSchema()\n",
        "            print(\"\\nTop 5 rows (Spark parquet):\")\n",
        "            df.show(5, truncate=False)\n",
        "\n",
        "    def _build_zip_deliverable(self) -> str:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - None\n",
        "        Output:\n",
        "          - zip_path: path to a zip file containing outputs + state\n",
        "        \"\"\"\n",
        "        zip_path = os.path.join(self._config.base_dir, \"deliverable_covid_challenge.zip\")\n",
        "\n",
        "        # Ensure we don't append to an old zip\n",
        "        if os.path.exists(zip_path):\n",
        "            os.remove(zip_path)\n",
        "\n",
        "        def _iter_files(root: str) -> Iterable[Tuple[str, str]]:\n",
        "            for dirpath, _, filenames in os.walk(root):\n",
        "                for f in filenames:\n",
        "                    abs_path = os.path.join(dirpath, f)\n",
        "                    rel_path = os.path.relpath(abs_path, self._config.base_dir)\n",
        "                    yield abs_path, rel_path\n",
        "\n",
        "        with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "            for folder in [self._config.spark_parquet_dir, self._config.pandas_parquet_dir, self._config.state_dir]:\n",
        "                if os.path.exists(folder):\n",
        "                    for abs_path, rel_path in _iter_files(folder):\n",
        "                        zf.write(abs_path, arcname=rel_path)\n",
        "\n",
        "            # Also include a lightweight README with run instructions\n",
        "            readme_text = self._render_readme_text()\n",
        "            readme_path = os.path.join(self._config.base_dir, \"README_DELIVERABLE.txt\")\n",
        "            with open(readme_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(readme_text)\n",
        "            zf.write(readme_path, arcname=\"README_DELIVERABLE.txt\")\n",
        "\n",
        "        return zip_path\n",
        "\n",
        "    def _render_readme_text(self) -> str:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - None\n",
        "        Output:\n",
        "          - A short README string for the zip deliverable.\n",
        "        \"\"\"\n",
        "        return (\n",
        "            \"Sr Data Engineer Challenge - Covid-19 (Kaggle)\\n\"\n",
        "            \"================================================\\n\\n\"\n",
        "            \"This ZIP contains:\\n\"\n",
        "            \"- parquet_spark/: Parquet outputs generated with Spark using an RDD-based CSV read.\\n\"\n",
        "            \"- parquet_pandas/: Parquet outputs generated with Pandas.\\n\"\n",
        "            \"- state/: Delta control state (seen row key hashes) so each run processes only novelties.\\n\\n\"\n",
        "            \"Key design notes:\\n\"\n",
        "            \"- Type enforcement is done at the last layer before writing Parquet.\\n\"\n",
        "            \"- Delta logic uses a SHA-256 hash of a composite key per dataset.\\n\"\n",
        "            \"- Runs are append-only for delta records; state is overwritten with the merged set of keys.\\n\"\n",
        "        )\n",
        "\n",
        "    def _print_der_diagram(self) -> None:\n",
        "        \"\"\"\n",
        "        Input:\n",
        "          - None\n",
        "        Output:\n",
        "          - None. Prints a DER diagram suggestion in Mermaid format.\n",
        "        Notes:\n",
        "          - The Kaggle files are analytical snapshots rather than a normalized OLTP model.\n",
        "          - For a clean relational model, we separate dimensions and fact tables.\n",
        "        \"\"\"\n",
        "        print(\"\\n============================================================\")\n",
        "        print(\"DER (Mermaid) - Suggested Analytical Model\")\n",
        "        print(\"============================================================\\n\")\n",
        "\n",
        "        mermaid = r\"\"\"\n",
        "derDiagram\n",
        "    DIM_DATE {\n",
        "      date DATE PK\n",
        "    }\n",
        "\n",
        "    DIM_REGION {\n",
        "      region_id STRING PK\n",
        "      country STRING\n",
        "      province STRING\n",
        "      who_region STRING\n",
        "      continent STRING\n",
        "      lat DOUBLE\n",
        "      lon DOUBLE\n",
        "      population BIGINT\n",
        "    }\n",
        "\n",
        "    FACT_DAILY_REGION_METRICS {\n",
        "      date DATE FK\n",
        "      region_id STRING FK\n",
        "      confirmed BIGINT\n",
        "      deaths BIGINT\n",
        "      recovered BIGINT\n",
        "      active BIGINT\n",
        "      new_cases BIGINT\n",
        "      new_deaths BIGINT\n",
        "      new_recovered BIGINT\n",
        "    }\n",
        "\n",
        "    FACT_COUNTRY_LATEST {\n",
        "      region_id STRING FK\n",
        "      confirmed BIGINT\n",
        "      deaths BIGINT\n",
        "      recovered BIGINT\n",
        "      active BIGINT\n",
        "      new_cases BIGINT\n",
        "      new_deaths BIGINT\n",
        "      new_recovered BIGINT\n",
        "      confirmed_last_week BIGINT\n",
        "      week_change BIGINT\n",
        "      week_pct_increase DOUBLE\n",
        "    }\n",
        "\n",
        "    DIM_DATE ||--o{ FACT_DAILY_REGION_METRICS : \"has\"\n",
        "    DIM_REGION ||--o{ FACT_DAILY_REGION_METRICS : \"tracks\"\n",
        "    DIM_REGION ||--o{ FACT_COUNTRY_LATEST : \"summarizes\"\n",
        "\"\"\"\n",
        "        print(mermaid)\n",
        "        print(\n",
        "            \"How to use it:\\n\"\n",
        "            \"- Paste the Mermaid block into a Mermaid editor (or Markdown that supports Mermaid)\\n\"\n",
        "            \"- This is a recommended normalized analytical model based on the dataset semantics.\\n\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Execute the pipeline\n",
        "# ============================================================\n",
        "\n",
        "# IMPORTANT: Keep the dataset id stable (matches the challenge requirement)\n",
        "DATASET_ID = \"imdevskp/corona-virus-report\"\n",
        "\n",
        "config = AppConfig(base_dir=\"/content/covid_challenge\")\n",
        "downloader = KaggleDownloader(dataset=DATASET_ID)\n",
        "catalog = DatasetCatalog()\n",
        "spark_factory = SparkSessionFactory()\n",
        "\n",
        "runner = PipelineRunner(\n",
        "    config=config,\n",
        "    downloader=downloader,\n",
        "    catalog=catalog,\n",
        "    spark_factory=spark_factory,\n",
        ")\n",
        "\n",
        "runner.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQiNeK5-KN-L",
        "outputId": "50cb4026-9802-453e-edc3-9cd539df1371"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/imdevskp/corona-virus-report\n",
            "License(s): other\n",
            "Extracted dataset to: /content/covid_challenge/raw/extracted\n",
            "============================================================\n",
            "Running ingestion (Spark + RDD) and ingestion (Pandas)\n",
            "Delta logic: only novelties are appended each run\n",
            "============================================================\n",
            "\n",
            "--- Spark report (RDD-based CSV read) ---\n",
            "[Spark] covid_19_clean_complete: read=49068 | delta_written=49068\n",
            "[Spark] country_wise_latest: read=187 | delta_written=187\n",
            "[Spark] day_wise: read=188 | delta_written=188\n",
            "[Spark] full_grouped: read=35156 | delta_written=35156\n",
            "[Spark] usa_county_wise: read=627920 | delta_written=627920\n",
            "[Spark] worldometer_data: read=209 | delta_written=209\n",
            "\n",
            "--- Pandas report ---\n",
            "[Pandas] covid_19_clean_complete: read=49068 | delta_written=49068\n",
            "[Pandas] country_wise_latest: read=187 | delta_written=187\n",
            "[Pandas] day_wise: read=188 | delta_written=188\n",
            "[Pandas] full_grouped: read=35156 | delta_written=35156\n",
            "[Pandas] usa_county_wise: read=627920 | delta_written=627920\n",
            "[Pandas] worldometer_data: read=209 | delta_written=209\n",
            "\n",
            "Schema check (Spark parquet): covid_19_clean_complete\n",
            "root\n",
            " |-- Province/State: string (nullable = true)\n",
            " |-- Country/Region: string (nullable = true)\n",
            " |-- Lat: double (nullable = true)\n",
            " |-- Long: double (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            " |-- Confirmed: long (nullable = true)\n",
            " |-- Deaths: long (nullable = true)\n",
            " |-- Recovered: long (nullable = true)\n",
            " |-- Active: long (nullable = true)\n",
            " |-- WHO Region: string (nullable = true)\n",
            " |-- row_key_hash: string (nullable = true)\n",
            "\n",
            "\n",
            "Top 5 rows (Spark parquet):\n",
            "+--------------+--------------+-------------------+---------+----------+---------+------+---------+------+----------+----------------------------------------------------------------+\n",
            "|Province/State|Country/Region|Lat                |Long     |Date      |Confirmed|Deaths|Recovered|Active|WHO Region|row_key_hash                                                    |\n",
            "+--------------+--------------+-------------------+---------+----------+---------+------+---------+------+----------+----------------------------------------------------------------+\n",
            "|NULL          |Liberia       |6.4280550000000005 |-9.429499|2020-04-28|141      |16    |45       |80    |Africa    |c778bbb28244026cc16698cf120440f6dffdeb9771cdfe229a9747daa7784d2c|\n",
            "|NULL          |Liechtenstein |47.14              |9.55     |2020-04-28|82       |1     |80       |1     |Europe    |be558dbec54c97eb88a156c95de6a361e0da67cbade6dfd9807730ac50be5099|\n",
            "|NULL          |Lithuania     |55.1694            |23.8813  |2020-04-28|1344     |44    |536      |764   |Europe    |696c821942f3444a760ae60a734d80e3ea91073f7f55710d2a4b497ab2efab42|\n",
            "|NULL          |Luxembourg    |49.8153            |6.1296   |2020-04-28|3741     |89    |3123     |529   |Europe    |c70a0792115fefa8bf81e34810d9833871c29201be7ed6d8cc57939a2534cd72|\n",
            "|NULL          |Madagascar    |-18.766947000000002|46.869107|2020-04-28|128      |0     |82       |46    |Africa    |e2d113654e494edbdd70c28214c05f57cb86fe39e0499b6ab68b1ed5d331119e|\n",
            "+--------------+--------------+-------------------+---------+----------+---------+------+---------+------+----------+----------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Deliverable ZIP created at: /content/covid_challenge/deliverable_covid_challenge.zip\n",
            "\n",
            "============================================================\n",
            "DER (Mermaid) - Suggested Analytical Model\n",
            "============================================================\n",
            "\n",
            "\n",
            "derDiagram\n",
            "    DIM_DATE {\n",
            "      date DATE PK\n",
            "    }\n",
            "\n",
            "    DIM_REGION {\n",
            "      region_id STRING PK\n",
            "      country STRING\n",
            "      province STRING\n",
            "      who_region STRING\n",
            "      continent STRING\n",
            "      lat DOUBLE\n",
            "      lon DOUBLE\n",
            "      population BIGINT\n",
            "    }\n",
            "\n",
            "    FACT_DAILY_REGION_METRICS {\n",
            "      date DATE FK\n",
            "      region_id STRING FK\n",
            "      confirmed BIGINT\n",
            "      deaths BIGINT\n",
            "      recovered BIGINT\n",
            "      active BIGINT\n",
            "      new_cases BIGINT\n",
            "      new_deaths BIGINT\n",
            "      new_recovered BIGINT\n",
            "    }\n",
            "\n",
            "    FACT_COUNTRY_LATEST {\n",
            "      region_id STRING FK\n",
            "      confirmed BIGINT\n",
            "      deaths BIGINT\n",
            "      recovered BIGINT\n",
            "      active BIGINT\n",
            "      new_cases BIGINT\n",
            "      new_deaths BIGINT\n",
            "      new_recovered BIGINT\n",
            "      confirmed_last_week BIGINT\n",
            "      week_change BIGINT\n",
            "      week_pct_increase DOUBLE\n",
            "    }\n",
            "\n",
            "    DIM_DATE ||--o{ FACT_DAILY_REGION_METRICS : \"has\"\n",
            "    DIM_REGION ||--o{ FACT_DAILY_REGION_METRICS : \"tracks\"\n",
            "    DIM_REGION ||--o{ FACT_COUNTRY_LATEST : \"summarizes\"\n",
            "\n",
            "How to use it:\n",
            "- Paste the Mermaid block into a Mermaid editor (or Markdown that supports Mermaid)\n",
            "- This is a recommended normalized analytical model based on the dataset semantics.\n",
            "\n",
            "\n",
            "Stopping Spark session...\n",
            "Spark stopped.\n"
          ]
        }
      ]
    }
  ]
}
